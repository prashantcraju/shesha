{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b6-YvfJbnSO"
      },
      "source": [
        "## Shesha Tutorial: Measuring Representational Drift\n",
        "\n",
        "This notebook demonstrates how to use `shesha.rdm_drift` and `shesha.rdm_similarity` to measure representational drift under various perturbations:\n",
        "\n",
        "1. **Gaussian noise injection** - Adding noise to model weights\n",
        "2. **LoRA adapters** - Simulating fine-tuning perturbations\n",
        "\n",
        "**What you'll learn:**\n",
        "- How to measure drift between two representations\n",
        "- How drift scales with perturbation magnitude\n",
        "- How to interpret drift values\n",
        "\n",
        "**Requirements:**\n",
        "```bash\n",
        "pip install shesha-geometry transformers torch peft datasets\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym381a4KbnSP"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Install dependencies\n",
        "# !pip install shesha-geometry transformers torch peft datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm22NrxTbnSQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "\n",
        "# Shesha\n",
        "import shesha\n",
        "\n",
        "# Optional: LoRA support\n",
        "try:\n",
        "    from peft import get_peft_model, LoraConfig, TaskType\n",
        "    HAS_PEFT = True\n",
        "except ImportError:\n",
        "    HAS_PEFT = False\n",
        "    print(\"peft not installed - LoRA experiments will be skipped\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "SEED = 320\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.bfloat16 if DEVICE == \"cuda\" else torch.float32\n",
        "BATCH_SIZE = 32 if DEVICE == \"cuda\" else 8\n",
        "N_SAMPLES = 200\n",
        "MAX_SEQ_LEN = 64\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "print(f\"SHESHA version: {shesha.__version__}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Available functions: {[x for x in dir(shesha) if not x.startswith('_')]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWrCM0uJbnSQ"
      },
      "source": [
        "## 2. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s6fgxRTbnSR"
      },
      "outputs": [],
      "source": [
        "print(\"Loading SST-2 dataset...\")\n",
        "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "# Sample balanced subset\n",
        "df = pd.DataFrame({'text': dataset['sentence'], 'label': dataset['label']})\n",
        "df = df.groupby('label').apply(lambda x: x.sample(N_SAMPLES // 2, random_state=SEED)).reset_index(drop=True)\n",
        "texts = df['text'].tolist()\n",
        "labels = df['label'].values\n",
        "\n",
        "print(f\"Loaded {len(texts)} samples\")\n",
        "print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "print(f\"\\nExample: '{texts[0][:60]}...'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4_BTRq0bnSR"
      },
      "source": [
        "## 3. Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQWDIO6rbnSR"
      },
      "outputs": [],
      "source": [
        "def load_model(model_name):\n",
        "    \"\"\"Load a causal LM model.\"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"left\"  # For causal LMs\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=DTYPE,\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "    ).to(DEVICE).eval()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def get_embeddings(model, tokenizer, texts, layer=-1):\n",
        "    \"\"\"Extract embeddings using mean pooling, L2 normalized.\"\"\"\n",
        "    model.eval()\n",
        "    all_vecs = []\n",
        "\n",
        "    for i in range(0, len(texts), BATCH_SIZE):\n",
        "        batch = texts[i:i+BATCH_SIZE]\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQ_LEN\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(**inputs, output_hidden_states=True, return_dict=True)\n",
        "            h = out.hidden_states[layer]\n",
        "\n",
        "            # Mean pooling over non-padding tokens\n",
        "            mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
        "            vecs = (h * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
        "            all_vecs.append(vecs.float().cpu().numpy())\n",
        "\n",
        "    emb = np.vstack(all_vecs)\n",
        "\n",
        "    # L2 normalize\n",
        "    norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
        "    emb = emb / np.maximum(norms, 1e-9)\n",
        "\n",
        "    return emb\n",
        "\n",
        "\n",
        "def inject_noise(model, alpha, seed=320):\n",
        "    \"\"\"Inject Gaussian noise into model parameters.\"\"\"\n",
        "    if alpha == 0:\n",
        "        return\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    with torch.no_grad():\n",
        "        for p in model.parameters():\n",
        "            if p.requires_grad and p.numel() > 1:\n",
        "                std = float(p.std().item())\n",
        "                if std > 0:\n",
        "                    noise = torch.randn_like(p) * (std * alpha)\n",
        "                    p.add_(noise)\n",
        "\n",
        "\n",
        "def get_state_dict(model):\n",
        "    \"\"\"Copy model state dict to CPU.\"\"\"\n",
        "    return {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "\n",
        "def load_state_dict(model, state_dict):\n",
        "    \"\"\"Restore model state dict.\"\"\"\n",
        "    device_state = {k: v.to(DEVICE) for k, v in state_dict.items()}\n",
        "    model.load_state_dict(device_state)\n",
        "\n",
        "\n",
        "print(\"Helper functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88wdeJySbnSS"
      },
      "source": [
        "## 4. Experiment 1: Gaussian Noise Drift\n",
        "\n",
        "We'll inject increasing amounts of Gaussian noise into model weights and measure how the representational geometry drifts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqOMeiJ5bnSS"
      },
      "outputs": [],
      "source": [
        "# Models to test (small for fast iteration)\n",
        "MODELS = [\n",
        "    \"HuggingFaceTB/SmolLM-135M\",\n",
        "    \"HuggingFaceTB/SmolLM-360M\",\n",
        "]\n",
        "\n",
        "# Noise levels (fraction of parameter std)\n",
        "NOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
        "\n",
        "noise_results = []\n",
        "\n",
        "for model_name in MODELS:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Load model\n",
        "    model, tokenizer = load_model(model_name)\n",
        "    clean_state = get_state_dict(model)\n",
        "\n",
        "    # Get clean embeddings\n",
        "    emb_clean = get_embeddings(model, tokenizer, texts)\n",
        "    print(f\"Clean embeddings: {emb_clean.shape}\")\n",
        "\n",
        "    # Test each noise level\n",
        "    print(f\"\\n{'Noise':<10} {'RDM_Sim':<12} {'Drift':<12} {'Stability':<12}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for alpha in NOISE_LEVELS:\n",
        "        # Restore clean weights\n",
        "        load_state_dict(model, clean_state)\n",
        "\n",
        "        # Inject noise\n",
        "        inject_noise(model, alpha, seed=SEED + int(alpha * 1000))\n",
        "\n",
        "        # Get noisy embeddings\n",
        "        emb_noisy = get_embeddings(model, tokenizer, texts)\n",
        "\n",
        "        # Compute drift metrics using SHESHA\n",
        "        rdm_sim = shesha.rdm_similarity(emb_clean, emb_noisy)\n",
        "        drift = shesha.rdm_drift(emb_clean, emb_noisy)\n",
        "        stability = shesha.feature_split(emb_noisy, n_splits=20, seed=SEED)\n",
        "\n",
        "        print(f\"{alpha:<10.2f} {rdm_sim:<12.4f} {drift:<12.4f} {stability:<12.4f}\")\n",
        "\n",
        "        noise_results.append({\n",
        "            'model': model_name.split('/')[-1],\n",
        "            'noise_level': alpha,\n",
        "            'rdm_similarity': rdm_sim,\n",
        "            'rdm_drift': drift,\n",
        "            'stability': stability,\n",
        "        })\n",
        "\n",
        "    # Cleanup\n",
        "    del model, tokenizer, clean_state, emb_clean\n",
        "    torch.cuda.empty_cache() if DEVICE == \"cuda\" else None\n",
        "\n",
        "df_noise = pd.DataFrame(noise_results)\n",
        "print(f\"\\n\\nCollected {len(df_noise)} measurements\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmV7ZzODbnST"
      },
      "source": [
        "## 5. Experiment 2: LoRA Adapter Drift\n",
        "\n",
        "We'll apply LoRA adapters with varying ranks and initialization scales to simulate fine-tuning perturbations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44b4AbGHbnST"
      },
      "outputs": [],
      "source": [
        "if not HAS_PEFT:\n",
        "    print(\"Skipping LoRA experiment (peft not installed)\")\n",
        "    df_lora = pd.DataFrame()\n",
        "else:\n",
        "    # LoRA configurations\n",
        "    LORA_CONFIGS = [\n",
        "        # Varying rank at moderate init (simulates \"how much capacity\")\n",
        "        {\"rank\": 1, \"alpha\": 2, \"init_scale\": 0.05},\n",
        "        {\"rank\": 4, \"alpha\": 8, \"init_scale\": 0.05},\n",
        "        {\"rank\": 8, \"alpha\": 16, \"init_scale\": 0.05},\n",
        "        {\"rank\": 16, \"alpha\": 32, \"init_scale\": 0.05},\n",
        "        {\"rank\": 32, \"alpha\": 64, \"init_scale\": 0.05},\n",
        "\n",
        "        # Varying init at fixed rank (simulates \"how much training\")\n",
        "        {\"rank\": 8, \"alpha\": 16, \"init_scale\": 0.01},\n",
        "        {\"rank\": 8, \"alpha\": 16, \"init_scale\": 0.05},\n",
        "        {\"rank\": 8, \"alpha\": 16, \"init_scale\": 0.1},\n",
        "    ]\n",
        "\n",
        "    lora_results = []\n",
        "\n",
        "    for model_name in MODELS:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Model: {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Load base model and get clean embeddings\n",
        "        model, tokenizer = load_model(model_name)\n",
        "        emb_clean = get_embeddings(model, tokenizer, texts)\n",
        "        print(f\"Clean embeddings: {emb_clean.shape}\")\n",
        "        del model\n",
        "        torch.cuda.empty_cache() if DEVICE == \"cuda\" else None\n",
        "\n",
        "        print(f\"\\n{'Rank':<8} {'Alpha':<8} {'RDM_Sim':<12} {'Drift':<12} {'Stability':<12}\")\n",
        "        print(\"-\" * 55)\n",
        "\n",
        "        for config in LORA_CONFIGS:\n",
        "            rank = config[\"rank\"]\n",
        "            alpha = config[\"alpha\"]\n",
        "            init_scale = config[\"init_scale\"]\n",
        "\n",
        "            # Load fresh model\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=DTYPE,\n",
        "                trust_remote_code=True,\n",
        "            ).to(DEVICE)\n",
        "\n",
        "            # Apply LoRA\n",
        "            try:\n",
        "                lora_config = LoraConfig(\n",
        "                    r=rank,\n",
        "                    lora_alpha=alpha,\n",
        "                    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "                    lora_dropout=0.0,\n",
        "                    bias=\"none\",\n",
        "                    task_type=TaskType.CAUSAL_LM\n",
        "                )\n",
        "                model = get_peft_model(model, lora_config)\n",
        "\n",
        "                # Initialize LoRA weights with random values\n",
        "                torch.manual_seed(SEED + rank)\n",
        "                with torch.no_grad():\n",
        "                    for name, param in model.named_parameters():\n",
        "                        if \"lora_\" in name:\n",
        "                            param.data = torch.randn_like(param) * init_scale\n",
        "\n",
        "                model.eval()\n",
        "\n",
        "                # Get embeddings\n",
        "                emb_lora = get_embeddings(model, tokenizer, texts)\n",
        "\n",
        "                # Compute drift\n",
        "                rdm_sim = shesha.rdm_similarity(emb_clean, emb_lora)\n",
        "                drift = shesha.rdm_drift(emb_clean, emb_lora)\n",
        "                stability = shesha.feature_split(emb_lora, n_splits=20, seed=SEED)\n",
        "\n",
        "                print(f\"{rank:<8} {alpha:<8} {rdm_sim:<12.4f} {drift:<12.4f} {stability:<12.4f}\")\n",
        "\n",
        "                lora_results.append({\n",
        "                    'model': model_name.split('/')[-1],\n",
        "                    'lora_rank': rank,\n",
        "                    'lora_alpha': alpha,\n",
        "                    'rdm_similarity': rdm_sim,\n",
        "                    'rdm_drift': drift,\n",
        "                    'stability': stability,\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"{rank:<8} {alpha:<8} ERROR: {e}\")\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache() if DEVICE == \"cuda\" else None\n",
        "\n",
        "        del emb_clean, tokenizer\n",
        "\n",
        "    df_lora = pd.DataFrame(lora_results)\n",
        "    print(f\"\\n\\nCollected {len(df_lora)} measurements\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le8S-QMecaJL"
      },
      "outputs": [],
      "source": [
        "if not HAS_PEFT:\n",
        "    print(\"Skipping LoRA experiment (peft not installed)\")\n",
        "    df_lora = pd.DataFrame()\n",
        "else:\n",
        "    # LoRA configurations\n",
        "    LORA_CONFIGS = [\n",
        "        {\"rank\": 1, \"alpha\": 2, \"init_scale\": 0.01},\n",
        "        {\"rank\": 2, \"alpha\": 4, \"init_scale\": 0.01},\n",
        "        {\"rank\": 4, \"alpha\": 8, \"init_scale\": 0.01},\n",
        "        {\"rank\": 8, \"alpha\": 16, \"init_scale\": 0.01},\n",
        "        {\"rank\": 16, \"alpha\": 32, \"init_scale\": 0.01},\n",
        "        {\"rank\": 32, \"alpha\": 64, \"init_scale\": 0.01},\n",
        "    ]\n",
        "\n",
        "\n",
        "    lora_results = []\n",
        "\n",
        "    for model_name in MODELS:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Model: {model_name}\")\n",
        "\n",
        "        # 1. Load Base Model\n",
        "        base_model, tokenizer = load_model(model_name)\n",
        "\n",
        "        # Get clean embeddings from base model\n",
        "        emb_clean = get_embeddings(base_model, tokenizer, texts)\n",
        "        print(f\"Clean embeddings: {emb_clean.shape}\")\n",
        "\n",
        "        print(f\"\\n{'Rank':<8} {'Alpha':<8} {'RDM_Sim':<12} {'Drift':<12} {'Stability':<12}\")\n",
        "        print(\"-\" * 55)\n",
        "\n",
        "        for config in LORA_CONFIGS:\n",
        "            rank = config[\"rank\"]\n",
        "            alpha = config[\"alpha\"]\n",
        "            init_scale = config[\"init_scale\"]\n",
        "\n",
        "            try:\n",
        "                # 2. Dynamic Target Modules (Optional safety check)\n",
        "                # You might need a helper here to determine targets based on model_name\n",
        "                target_modules = [\"q_proj\", \"v_proj\"]\n",
        "\n",
        "                lora_config = LoraConfig(\n",
        "                    r=rank,\n",
        "                    lora_alpha=alpha,\n",
        "                    target_modules=target_modules,\n",
        "                    lora_dropout=0.0,\n",
        "                    bias=\"none\",\n",
        "                    task_type=TaskType.CAUSAL_LM\n",
        "                )\n",
        "\n",
        "                # 3. Apply Adapter to existing model in-memory\n",
        "                # This wraps the base model without reloading weights\n",
        "                peft_model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "                # 4. Initialization\n",
        "                torch.manual_seed(SEED + rank)\n",
        "                with torch.no_grad():\n",
        "                    for name, param in peft_model.named_parameters():\n",
        "                        if \"lora_A\" in name:\n",
        "                            # Initialize A with noise\n",
        "                            param.data = torch.randn_like(param) * init_scale\n",
        "                        elif \"lora_B\" in name:\n",
        "                            # Initialize B with noise (For Drift Experiment)\n",
        "                            # NOTE: Standard LoRA keeps this at 0.\n",
        "                            # To simulate drift, noise is fine, but be aware\n",
        "                            # you are breaking the identity property.\n",
        "                            param.data = torch.randn_like(param)\n",
        "\n",
        "                peft_model.eval()\n",
        "\n",
        "                # Get embeddings\n",
        "                emb_lora = get_embeddings(peft_model, tokenizer, texts)\n",
        "\n",
        "                # Compute metrics\n",
        "                rdm_sim = shesha.rdm_similarity(emb_clean, emb_lora)\n",
        "                drift = shesha.rdm_drift(emb_clean, emb_lora)\n",
        "                stability = shesha.feature_split(emb_lora, n_splits=20, seed=SEED)\n",
        "\n",
        "                print(f\"{rank:<8} {alpha:<8} {rdm_sim:<12.4f} {drift:<12.4f} {stability:<12.4f}\")\n",
        "\n",
        "                lora_results.append({\n",
        "                    'model': model_name.split('/')[-1],\n",
        "                    'lora_rank': rank,\n",
        "                    'lora_alpha': alpha,\n",
        "                    'rdm_similarity': rdm_sim,\n",
        "                    'rdm_drift': drift,\n",
        "                    'stability': stability,\n",
        "                })\n",
        "\n",
        "                # This removes the LoRA layers so the next iteration starts clean\n",
        "                peft_model.unload()\n",
        "                del peft_model\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"{rank:<8} {alpha:<8} ERROR: {e}\")\n",
        "                # Fallback cleanup just in case\n",
        "                if 'peft_model' in locals():\n",
        "                    try: peft_model.unload()\n",
        "                    except: pass\n",
        "\n",
        "            torch.cuda.empty_cache() if DEVICE == \"cuda\" else None\n",
        "\n",
        "        # Clean up base model before next model in outer loop\n",
        "        del base_model, tokenizer\n",
        "        torch.cuda.empty_cache() if DEVICE == \"cuda\" else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCTpv-sBbnSU"
      },
      "source": [
        "## 6. Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0GdD57BbnSU"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"GAUSSIAN NOISE RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(df_noise.to_string(index=False))\n",
        "\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(\"MEAN DRIFT BY NOISE LEVEL\")\n",
        "print(\"=\"*60)\n",
        "noise_summary = df_noise.groupby('noise_level')[['rdm_similarity', 'rdm_drift', 'stability']].mean()\n",
        "print(noise_summary.to_string())\n",
        "\n",
        "if len(df_lora) > 0:\n",
        "    print(f\"\\n\\n{'='*60}\")\n",
        "    print(\"LORA RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(df_lora.to_string(index=False))\n",
        "\n",
        "    print(f\"\\n\\n{'='*60}\")\n",
        "    print(\"MEAN DRIFT BY LORA RANK\")\n",
        "    print(\"=\"*60)\n",
        "    lora_summary = df_lora.groupby('lora_rank')[['rdm_similarity', 'rdm_drift', 'stability']].mean()\n",
        "    print(lora_summary.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCSPZzPabnSV"
      },
      "source": [
        "## 7. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHNeKpLWbnSV"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Plot 1: Drift vs Noise Level\n",
        "ax = axes[0, 0]\n",
        "for model in df_noise['model'].unique():\n",
        "    subset = df_noise[df_noise['model'] == model]\n",
        "    ax.plot(subset['noise_level'], subset['rdm_drift'], 'o-', label=model, markersize=6)\n",
        "ax.set_xlabel('Noise Level (fraction of param std)')\n",
        "ax.set_ylabel('RDM Drift')\n",
        "ax.set_title('Representational Drift vs Gaussian Noise')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Similarity vs Noise Level\n",
        "ax = axes[0, 1]\n",
        "for model in df_noise['model'].unique():\n",
        "    subset = df_noise[df_noise['model'] == model]\n",
        "    ax.plot(subset['noise_level'], subset['rdm_similarity'], 's-', label=model, markersize=6)\n",
        "ax.set_xlabel('Noise Level (fraction of param std)')\n",
        "ax.set_ylabel('RDM Similarity')\n",
        "ax.set_title('RDM Similarity vs Gaussian Noise')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim([0, 1.05])\n",
        "\n",
        "# Plot 3: Drift vs LoRA Rank\n",
        "ax = axes[1, 0]\n",
        "if len(df_lora) > 0:\n",
        "    for model in df_lora['model'].unique():\n",
        "        subset = df_lora[df_lora['model'] == model]\n",
        "        ax.plot(subset['lora_rank'], subset['rdm_drift'], 'o-', label=model, markersize=6)\n",
        "    ax.set_xlabel('LoRA Rank')\n",
        "    ax.set_ylabel('RDM Drift')\n",
        "    ax.set_title('Representational Drift vs LoRA Rank')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xscale('log', base=2)\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'LoRA experiment skipped\\n(peft not installed)',\n",
        "            ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
        "    ax.set_title('LoRA Drift (skipped)')\n",
        "\n",
        "# Plot 4: Stability vs Drift (all experiments)\n",
        "ax = axes[1, 1]\n",
        "ax.scatter(df_noise['rdm_drift'], df_noise['stability'],\n",
        "           alpha=0.7, label='Gaussian Noise', s=50, c='blue')\n",
        "if len(df_lora) > 0:\n",
        "    ax.scatter(df_lora['rdm_drift'], df_lora['stability'],\n",
        "               alpha=0.7, label='LoRA', s=50, c='orange', marker='s')\n",
        "ax.set_xlabel('RDM Drift (from clean)')\n",
        "ax.set_ylabel('Feature-Split Stability')\n",
        "ax.set_title('Internal Stability vs Drift')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('drift_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\nFigure saved as 'drift_analysis.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHwV4orqbnSV"
      },
      "source": [
        "## 8. Interpretation Guide\n",
        "\n",
        "### RDM Drift Values\n",
        "\n",
        "| Drift | Interpretation |\n",
        "|-------|----------------|\n",
        "| 0.0 | Identical geometry |\n",
        "| 0.0-0.1 | Minimal drift (very similar) |\n",
        "| 0.1-0.3 | Moderate drift |\n",
        "| 0.3-0.5 | Substantial drift |\n",
        "| 0.5+ | Major geometric change |\n",
        "| 1.0 | Uncorrelated (random) |\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Gaussian Noise**: Drift increases monotonically with noise level\n",
        "2. **LoRA Rank**: Higher ranks = more parameters = more potential drift\n",
        "3. **Stability vs Drift**: Internal stability often decreases as drift increases\n",
        "\n",
        "### When to Use Each Metric\n",
        "\n",
        "- **`rdm_similarity`**: When you want similarity (higher = more similar)\n",
        "- **`rdm_drift`**: When you want to measure change (higher = more change)\n",
        "- **`feature_split`**: For internal consistency of a single representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOLJq54JbnSV"
      },
      "source": [
        "## 9. Quick Reference\n",
        "\n",
        "```python\n",
        "import shesha\n",
        "\n",
        "# Measure similarity between two representations\n",
        "similarity = shesha.rdm_similarity(X_before, X_after)\n",
        "# Returns: float in [-1, 1], higher = more similar geometry\n",
        "\n",
        "# Measure drift between two representations\n",
        "drift = shesha.rdm_drift(X_before, X_after)\n",
        "# Returns: float in [0, 2], where 0 = identical, 1 = uncorrelated\n",
        "\n",
        "# Use Pearson instead of Spearman\n",
        "drift = shesha.rdm_drift(X_before, X_after, method='pearson')\n",
        "\n",
        "# Use different distance metric\n",
        "drift = shesha.rdm_drift(X_before, X_after, metric='euclidean')\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
