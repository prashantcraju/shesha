{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY2gp_J__lsy"
      },
      "source": [
        "# Shesha Tutorial: LLM Embedding Analysis\n",
        "\n",
        "This notebook demonstrates how to use Shesha to analyze the geometric stability of language model embeddings.\n",
        "\n",
        "**What you'll learn:**\n",
        "- How to extract embeddings from transformer models\n",
        "- How to measure representation stability with `feature_split`\n",
        "- How to compare stability across layers and models\n",
        "- How to use supervised alignment to measure task relevance\n",
        "\n",
        "**Requirements:**\n",
        "```bash\n",
        "pip install shesha-geometry transformers torch datasets\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-UNnypc_ls0"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Install dependencies\n",
        "# !pip install shesha-geometry transformers torch datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFun6Lyg_ls0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Shesha\n",
        "import shesha\n",
        "\n",
        "# Configuration\n",
        "SEED = 320\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Shesha version: {shesha.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5Nq1k73_ls1"
      },
      "source": [
        "## 2. Load Model and Tokenizer\n",
        "\n",
        "We'll use BERT-base as an example, but this works with any HuggingFace transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urIhfuLM_ls1"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME, output_hidden_states=True)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded: {model.config.num_hidden_layers} layers, {model.config.hidden_size} hidden dim\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLSBb17z_ls1"
      },
      "source": [
        "## 3. Load a Text Dataset\n",
        "\n",
        "We'll use a subset of the SST-2 sentiment dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2JeX0O3_ls1"
      },
      "outputs": [],
      "source": [
        "# Load SST-2 dataset\n",
        "print(\"Loading SST-2 dataset...\")\n",
        "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "# Take a subset for speed\n",
        "N_SAMPLES = 500\n",
        "texts = dataset[\"sentence\"][:N_SAMPLES]\n",
        "labels = np.array(dataset[\"label\"][:N_SAMPLES])\n",
        "\n",
        "print(f\"Loaded {len(texts)} samples\")\n",
        "print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "print(f\"\\nExample: '{texts[0]}' -> {labels[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxNy6kaU_ls2"
      },
      "source": [
        "## 4. Extract Embeddings\n",
        "\n",
        "We'll extract [CLS] token embeddings from each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGAzeXn1_ls2"
      },
      "outputs": [],
      "source": [
        "def extract_embeddings(texts, model, tokenizer, batch_size=32):\n",
        "    \"\"\"Extract [CLS] embeddings from all layers.\"\"\"\n",
        "    all_hidden_states = [[] for _ in range(model.config.num_hidden_layers + 1)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=128,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(DEVICE)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # Extract [CLS] token (index 0) from each layer\n",
        "            for layer_idx, hidden_state in enumerate(outputs.hidden_states):\n",
        "                cls_embedding = hidden_state[:, 0, :].cpu().numpy()\n",
        "                all_hidden_states[layer_idx].append(cls_embedding)\n",
        "\n",
        "    # Concatenate batches\n",
        "    return [np.vstack(layer) for layer in all_hidden_states]\n",
        "\n",
        "print(\"Extracting embeddings...\")\n",
        "layer_embeddings = extract_embeddings(texts, model, tokenizer)\n",
        "print(f\"Extracted embeddings from {len(layer_embeddings)} layers\")\n",
        "print(f\"Shape per layer: {layer_embeddings[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0rO68wN_ls2"
      },
      "source": [
        "## 5. Measure Stability Across Layers\n",
        "\n",
        "Use `shesha.feature_split` to measure geometric stability at each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f7gwqra_ls2"
      },
      "outputs": [],
      "source": [
        "# Compute stability for each layer\n",
        "stabilities = []\n",
        "\n",
        "print(f\"{'Layer':<10} {'Stability':>10}\")\n",
        "print(\"-\" * 22)\n",
        "\n",
        "for layer_idx, embeddings in enumerate(layer_embeddings):\n",
        "    stability = shesha.feature_split(embeddings, n_splits=30, seed=SEED)\n",
        "    stabilities.append(stability)\n",
        "    print(f\"{layer_idx:<10} {stability:>10.3f}\")\n",
        "\n",
        "print(f\"\\nMean stability: {np.mean(stabilities):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U3wamhS_ls2"
      },
      "source": [
        "## 6. Measure Task Alignment (Supervised)\n",
        "\n",
        "Use `shesha.supervised_alignment` to measure how well each layer's geometry aligns with sentiment labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4w2jV-c_ls2"
      },
      "outputs": [],
      "source": [
        "# Compute supervised alignment for each layer\n",
        "alignments = []\n",
        "\n",
        "print(f\"{'Layer':<10} {'Stability':>10} {'Alignment':>10}\")\n",
        "print(\"-\" * 32)\n",
        "\n",
        "for layer_idx, embeddings in enumerate(layer_embeddings):\n",
        "    alignment = shesha.supervised_alignment(embeddings, labels, seed=SEED)\n",
        "    alignments.append(alignment)\n",
        "    print(f\"{layer_idx:<10} {stabilities[layer_idx]:>10.3f} {alignment:>10.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH_rbljP_ls2"
      },
      "source": [
        "## 7. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js9qUKfr_ls2"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "layers = list(range(len(layer_embeddings)))\n",
        "\n",
        "# Plot 1: Stability across layers\n",
        "ax = axes[0]\n",
        "ax.plot(layers, stabilities, 'b-o', markersize=6)\n",
        "ax.set_xlabel('Layer')\n",
        "ax.set_ylabel('Stability (feature_split)')\n",
        "ax.set_title('Geometric Stability Across Layers')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xticks(layers)\n",
        "\n",
        "# Plot 2: Task alignment across layers\n",
        "ax = axes[1]\n",
        "ax.plot(layers, alignments, 'r-s', markersize=6)\n",
        "ax.set_xlabel('Layer')\n",
        "ax.set_ylabel('Task Alignment (supervised)')\n",
        "ax.set_title('Sentiment Alignment Across Layers')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xticks(layers)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('llm_layer_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Figure saved as 'llm_layer_analysis.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il0SvW-t_ls3"
      },
      "source": [
        "## 8. Compare Multiple Models\n",
        "\n",
        "Let's compare stability across different model sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5VGZ-SN_ls3"
      },
      "outputs": [],
      "source": [
        "def analyze_model(model_name, texts, labels):\n",
        "    \"\"\"Analyze a single model and return metrics.\"\"\"\n",
        "    print(f\"\\nAnalyzing {model_name}...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "    model = model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    # Extract final layer embeddings\n",
        "    layer_embeddings = extract_embeddings(texts, model, tokenizer)\n",
        "    final_layer = layer_embeddings[-1]\n",
        "\n",
        "    # Compute metrics\n",
        "    stability = shesha.feature_split(final_layer, n_splits=30, seed=SEED)\n",
        "    alignment = shesha.supervised_alignment(final_layer, labels, seed=SEED)\n",
        "\n",
        "    # Clean up\n",
        "    del model\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'stability': stability,\n",
        "        'alignment': alignment,\n",
        "        'hidden_dim': final_layer.shape[1]\n",
        "    }\n",
        "\n",
        "# Compare models (uncomment to run - takes a few minutes)\n",
        "# models_to_compare = [\n",
        "#     \"bert-base-uncased\",\n",
        "#     \"bert-large-uncased\",\n",
        "#     \"distilbert-base-uncased\",\n",
        "# ]\n",
        "#\n",
        "# results = [analyze_model(m, texts, labels) for m in models_to_compare]\n",
        "#\n",
        "# for r in results:\n",
        "#     print(f\"{r['model']:<30} stability={r['stability']:.3f}  alignment={r['alignment']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md6dc1N2_ls3"
      },
      "source": [
        "## 9. Monitor Fine-tuning Drift\n",
        "\n",
        "Shesha can track how representations change during fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCVs0vdf_ls3"
      },
      "outputs": [],
      "source": [
        "# Simulated fine-tuning drift example\n",
        "# In practice, you'd extract embeddings at each checkpoint\n",
        "\n",
        "print(\"Simulating fine-tuning drift monitoring...\")\n",
        "print(\"(In practice, extract embeddings at each training checkpoint)\\n\")\n",
        "\n",
        "# Simulate embeddings evolving during training\n",
        "rng = np.random.default_rng(SEED)\n",
        "base_embeddings = layer_embeddings[-1]  # Start from final BERT layer\n",
        "\n",
        "epochs = [0, 1, 2, 3, 4, 5]\n",
        "epoch_stabilities = []\n",
        "epoch_alignments = []\n",
        "\n",
        "for epoch in epochs:\n",
        "    # Simulate drift: add task-relevant signal + noise\n",
        "    signal_strength = epoch * 0.1\n",
        "    noise_strength = epoch * 0.05\n",
        "\n",
        "    # Add class-conditional signal\n",
        "    signal = np.zeros_like(base_embeddings)\n",
        "    signal[labels == 0] -= signal_strength\n",
        "    signal[labels == 1] += signal_strength\n",
        "\n",
        "    # Add noise\n",
        "    noise = rng.standard_normal(base_embeddings.shape) * noise_strength\n",
        "\n",
        "    epoch_embeddings = base_embeddings + signal + noise\n",
        "\n",
        "    stability = shesha.feature_split(epoch_embeddings, seed=SEED)\n",
        "    alignment = shesha.supervised_alignment(epoch_embeddings, labels, seed=SEED)\n",
        "\n",
        "    epoch_stabilities.append(stability)\n",
        "    epoch_alignments.append(alignment)\n",
        "\n",
        "    print(f\"Epoch {epoch}: stability={stability:.3f}, alignment={alignment:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfUMjXor_ls4"
      },
      "outputs": [],
      "source": [
        "# Plot training dynamics\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "\n",
        "ax.plot(epochs, epoch_stabilities, 'b-o', label='Stability', markersize=8)\n",
        "ax.plot(epochs, epoch_alignments, 'r-s', label='Task Alignment', markersize=8)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Representation Dynamics During Fine-tuning')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('finetuning_dynamics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmLZs1vP_ls4"
      },
      "source": [
        "## 10. Interpretation Guide\n",
        "\n",
        "### Stability (feature_split)\n",
        "- **High (> 0.5)**: Redundant, robust representations - geometry is consistent across feature subsets\n",
        "- **Low (< 0.2)**: Fragile representations - different features encode different structure\n",
        "\n",
        "### Task Alignment (supervised_alignment)\n",
        "- **High (> 0.3)**: Representations encode task-relevant structure\n",
        "- **Low (< 0.1)**: Representations don't reflect task labels\n",
        "\n",
        "### Common Patterns\n",
        "1. **Early layers**: Lower task alignment, variable stability\n",
        "2. **Middle layers**: Often highest stability (general features)\n",
        "3. **Final layers**: Higher task alignment (task-specific features)\n",
        "4. **Fine-tuning**: Alignment increases, stability may decrease (specialization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eAnYAsK_ls4"
      },
      "source": [
        "## 11. Quick Reference\n",
        "\n",
        "```python\n",
        "import shesha\n",
        "\n",
        "# Measure geometric stability (unsupervised)\n",
        "stability = shesha.feature_split(embeddings, n_splits=30, seed=320)\n",
        "\n",
        "# Measure task alignment (supervised)\n",
        "alignment = shesha.supervised_alignment(embeddings, labels, seed=320)\n",
        "\n",
        "# Quick separability check\n",
        "var_ratio = shesha.variance_ratio(embeddings, labels)\n",
        "\n",
        "# Alternative stability metrics\n",
        "sample_stab = shesha.sample_split(embeddings, seed=320)\n",
        "anchor_stab = shesha.anchor_stability(embeddings, seed=320)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
