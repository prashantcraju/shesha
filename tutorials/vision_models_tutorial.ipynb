{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUQVkzfECrZ3"
      },
      "source": [
        "# Shesha Tutorial: Vision Model Analysis\n",
        "\n",
        "This notebook demonstrates how to use Shesha to analyze the geometric stability of vision model representations.\n",
        "\n",
        "**What you'll learn:**\n",
        "- How to extract embeddings from vision models\n",
        "- How to compare stability across architectures (ResNet, ViT, etc.)\n",
        "- How to measure class separability\n",
        "\n",
        "**Requirements:**\n",
        "```bash\n",
        "pip install shesha-geometry torch torchvision\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ9eB1ayCrZ4"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf7-j6jbCrZ5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import shesha\n",
        "\n",
        "# Configuration\n",
        "SEED = 320\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GeUhf0ACrZ5"
      },
      "source": [
        "## 2. Load Dataset\n",
        "\n",
        "We'll use CIFAR-10 for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD-qWhY4CrZ5"
      },
      "outputs": [],
      "source": [
        "# ImageNet normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 test set\n",
        "dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Subset for speed\n",
        "N_SAMPLES = 1000\n",
        "indices = np.random.choice(len(dataset), N_SAMPLES, replace=False)\n",
        "subset = torch.utils.data.Subset(dataset, indices)\n",
        "dataloader = DataLoader(subset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f\"Dataset: {N_SAMPLES} images, 10 classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQNwhzPMCrZ5"
      },
      "source": [
        "## 3. Extract Embeddings from a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAPgjDDzCrZ5"
      },
      "outputs": [],
      "source": [
        "def extract_features(model, dataloader, device):\n",
        "    \"\"\"Extract features from penultimate layer.\"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Remove final classification layer\n",
        "    if hasattr(model, 'fc'):\n",
        "        model.fc = nn.Identity()\n",
        "    elif hasattr(model, 'classifier'):\n",
        "        model.classifier = nn.Identity()\n",
        "    elif hasattr(model, 'head'):\n",
        "        model.head = nn.Identity()\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            images = images.to(device)\n",
        "            output = model(images)\n",
        "\n",
        "            # Flatten if needed\n",
        "            if output.dim() > 2:\n",
        "                output = output.view(output.size(0), -1)\n",
        "\n",
        "            features.append(output.cpu().numpy())\n",
        "            labels.append(targets.numpy())\n",
        "\n",
        "    return np.vstack(features), np.concatenate(labels)\n",
        "\n",
        "# Test with ResNet-18\n",
        "print(\"Loading ResNet-18...\")\n",
        "resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
        "features, labels = extract_features(resnet, dataloader, DEVICE)\n",
        "print(f\"Feature shape: {features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF3oqmU4CrZ6"
      },
      "source": [
        "## 4. Analyze Single Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNoOzyAWCrZ6"
      },
      "outputs": [],
      "source": [
        "# Compute Shesha metrics\n",
        "stability = shesha.feature_split(features, n_splits=30, seed=SEED)\n",
        "alignment = shesha.supervised_alignment(features, labels, seed=SEED)\n",
        "var_ratio = shesha.variance_ratio(features, labels)\n",
        "\n",
        "print(\"ResNet-18 Metrics:\")\n",
        "print(f\"  Stability (feature_split):    {stability:.3f}\")\n",
        "print(f\"  Task Alignment (supervised):  {alignment:.3f}\")\n",
        "print(f\"  Variance Ratio:               {var_ratio:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aGXiQ-TCrZ6"
      },
      "source": [
        "## 5. Compare Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imF6AINrCrZ6"
      },
      "outputs": [],
      "source": [
        "def analyze_model(model_fn, model_name, dataloader):\n",
        "    \"\"\"Analyze a vision model.\"\"\"\n",
        "    print(f\"Analyzing {model_name}...\")\n",
        "    model = model_fn(weights='IMAGENET1K_V1')\n",
        "    features, labels = extract_features(model, dataloader, DEVICE)\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'stability': shesha.feature_split(features, seed=SEED),\n",
        "        'alignment': shesha.supervised_alignment(features, labels, seed=SEED),\n",
        "        'var_ratio': shesha.variance_ratio(features, labels),\n",
        "        'dim': features.shape[1]\n",
        "    }\n",
        "\n",
        "# Compare models\n",
        "model_configs = [\n",
        "    (models.resnet18, \"ResNet-18\"),\n",
        "    (models.resnet50, \"ResNet-50\"),\n",
        "    (models.vgg16, \"VGG-16\"),\n",
        "]\n",
        "\n",
        "results = []\n",
        "for model_fn, name in model_configs:\n",
        "    results.append(analyze_model(model_fn, name, dataloader))\n",
        "\n",
        "# Print comparison\n",
        "print(f\"\\n{'Model':<12} {'Dim':>6} {'Stability':>10} {'Alignment':>10} {'VarRatio':>10}\")\n",
        "print(\"-\" * 52)\n",
        "for r in results:\n",
        "    print(f\"{r['model']:<12} {r['dim']:>6} {r['stability']:>10.3f} {r['alignment']:>10.3f} {r['var_ratio']:>10.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qLiOfSACrZ6"
      },
      "source": [
        "## 6. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6UuZAX1CrZ6"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "models_names = [r['model'] for r in results]\n",
        "x = np.arange(len(models_names))\n",
        "width = 0.25\n",
        "\n",
        "ax.bar(x - width, [r['stability'] for r in results], width, label='Stability')\n",
        "ax.bar(x, [r['alignment'] for r in results], width, label='Alignment')\n",
        "ax.bar(x + width, [r['var_ratio'] for r in results], width, label='Var Ratio')\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Vision Model Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models_names)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('vision_comparison.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjDDWWTdCrZ6"
      },
      "source": [
        "## 7. Quick Reference\n",
        "\n",
        "```python\n",
        "import shesha\n",
        "\n",
        "# Extract features from your vision model\n",
        "features = model(images)  # shape: (n_samples, n_features)\n",
        "\n",
        "# Measure stability\n",
        "stability = shesha.feature_split(features, seed=320)\n",
        "\n",
        "# Measure task alignment\n",
        "alignment = shesha.supervised_alignment(features, labels, seed=320)\n",
        "\n",
        "# Quick separability check\n",
        "var_ratio = shesha.variance_ratio(features, labels)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}